TODOs:

- look at how much data each option (A, B, etc) has
- try classifiers other than GBC (especially for G, whose accuracy is the lowest)
- when calibrating remove the last few rows
- predict risk first

- look at misclassifieds
- should the same normalizer and imputer be used for training and prediction??

- look at correlation matrix. rf may be messed up now
- encode state and location


BUGs:

Finished todos:
- encode car value
- pandas pivot table avergae function needs to ignore nans
- try using the last line instead of the average
- add cheapest options features
- use classifiers
- write a pipe class that combines predictions
- write prediction code
- calibrate classifiers for each option separately
- read from condensed tables instead of creating new ones everytime

bug fixes:
- the leading factors are strange now
    <= pandas merge messes up the order
- pandas pivot table ignore "object" fields. i.e. even if we map from string to int (e.g. car_value), it will still be ignored due to the dtype still being object.
    <= created a new field.  pandas u r a piece of shit
- shows NaN where the data is NOT missing (e.g. day and homeowner for 10000000)
    <= i typed >0 instead of >-1, assuming that all values are positive. In fact, they are all non-negative.
- the prediction for every option is the same!! :(
    <= the same classifiers are used for all options.