TO-DO to-dos:

- remove outliers
- feature construction (no idea how to... yet)
    - identify all correlated features and take differences(?!)
- using area under the ROC curve to evaluate your model.
- get ROC to work with gradientboostingclassifier
- scoring problem? (http://www.kaggle.com/c/loan-default-prediction/forums/t/6956/binary-classification-vs-regression)
    1. create a CLASSIFIER

    2. Create score level from the outputs

    3. Calibrate these levels for (PD) -

    4.  Estimate the LGD (loss given default) as a joint probability (careful with bias estimations)


- better imputation (e.g. predict missing values)

- use kernel pca instead of pca
- vectorize f778 and f6 (discrete variables with lots of unique values)


================================================================================================
finished to-do:

- CV hell
- calibrate parameters of the entire pipeline
- standardize
- standardize before feature selection
- decomposition
- handle discrete values properly (http://scikit-learn.org/stable/modules/preprocessing.html, section 4.2.4)
- whiten (remove linear correlation between features)
- predict 0-loss as a binary classification problem first, then regress on the non-zero values
- use the same imputer and normalizer for classification and regression
- make the binary->reg one pipeline
- use GA_JJ instead of gridsearchcv
- self.minimize not setting in GA_JJ
- look at the roc auc for classification (at least 0.7 benchmark). Mine is 0.68. :(
- try out some classifier other than GBC
- fixed out-of-memory error with large params
- look at classifier accuracy
- try f528 - f274