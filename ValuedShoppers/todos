Todos:

- vw
- build auc into GBC

- cv where the validation data come from a different time period to simulate realistic kaggle test scores, since the
    kaggle test data set are for May~July whereas the training data come from March~April

- calibrate classifier parameters (after getting the  test score into the ball park)

- some offers are in the test set but never appeared in the training set:
    set([1221665,
         1190530,
         1221667,
         1230218,
         1221666,
         1203439,
         1220502,
         1220503,
         1221658,
         1219903,
         1219900,
         1213242,
         1221663])
     => could amending offers be the answer? i.e. replace company by the reputation of the company

finished todos: :)

- compress transaction history (all in memory)
- compress transaction using index file (not in memory) :)
- output compressed transactions in chunk
- compress in parallel
- assess feature importance
- train and predict using compressed transactions' data :)
- very basic imputation
- logistic regression...? (didn't really workout)
- CV function
- quantile regression...?
- use the information of number of repeats somehow...
- amend offers
- amend offers without blowing up memory

-------------------------------------

BUGS: :'( :'(


fixed BUGS: :D :D
- loop gets slower every time :'( => non-binary files don't support "random access". therefore, the later in the file,
    the longer file access takes